{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"-nmpVncVpZbo"},"outputs":[],"source":["! pip install keras-tcn -q\n","! pip install --user tensorflow-addons -q\n","print('installed')"]},{"cell_type":"code","execution_count":23,"metadata":{"cellView":"code","id":"EvzJW6E7F-yR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676667295774,"user_tz":420,"elapsed":173,"user":{"displayName":"dev developer","userId":"15432253627082125706"}},"outputId":"eafcdfc7-185e-406d-d56b-9a7fd6c0ce2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}],"source":["%tensorflow_version 2.10.1\n","\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import tensorflow.keras.backend as keras_backend\n","from tensorflow.keras import layers, models, losses, metrics,optimizers, utils\n","import tensorflow_addons as tfa\n","\n","import random\n","import pandas as pd \n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","\n","columns =  ['date','w_open_n', 'w_high_n', 'w_low_n', 'w_close_n', 'new_n', 'started_n','done_n', 'target_n', 'y_ystrdy_n']\n","\n","file_hlp_path = './helpdesk-ohlc.csv'\n","df_hlp = pd.read_csv(file_hlp_path,parse_dates=['date'],index_col=['date'], usecols=columns)\n","df_hlp.sort_index(inplace=True)\n","df_hlp.dropna(inplace=True)\n","\n","file_inc_path = './incident-ohlc.csv'\n","df_inc = pd.read_csv(file_inc_path,parse_dates=['date'],index_col=['date'], usecols=columns)\n","df_inc.sort_index(inplace=True)\n","df_inc.dropna(inplace=True)\n","\n","random.seed(67)\n","np.random.seed(67)\n","tf.random.set_seed(67)\n","tf.keras.backend.set_epsilon(1)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"8dXAQQEjv5oA","executionInfo":{"status":"ok","timestamp":1676666478257,"user_tz":420,"elapsed":424,"user":{"displayName":"dev developer","userId":"15432253627082125706"}}},"outputs":[],"source":["def convert_to_lookback(df, lookback_wind):\n","  y = df.dropna().target_n\n","  y_bsln = df.dropna().y_ystrdy_n\n","\n","  X = df.dropna().drop([\"target_n\",'y_ystrdy_n'], axis=1)\n","\n","  X= X.to_numpy()\n","  y= y.to_numpy()\n","  y_bsln = y_bsln.to_numpy()\n","\n","  X_lb = []\n","  y_lb = []\n","  y_bsln_lb = []\n","\n","  for i in tqdm(range(lookback_wind, len(X))):\n","    X_lb.append(X[i - lookback_wind:i])\n","    y_lb.append(y[i])\n","    y_bsln_lb.append(y_bsln[i])\n","\n","  X_lb = np.array(X_lb)\n","  y_lb = np.array(y_lb)\n","  y_bsln_lb = np.array(y_bsln_lb)\n","  return X_lb, y_lb, y_bsln_lb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d39NWui4x4yu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-l5S5-bmzAJI","executionInfo":{"status":"ok","timestamp":1676666481549,"user_tz":420,"elapsed":118,"user":{"displayName":"dev developer","userId":"15432253627082125706"}}},"outputs":[],"source":["# datareader.py\n","class Window:\n","  def __init__(self, X,y,y_bsln):\n","    self.X = X\n","    self.y = y\n","    self.y_bsln = y_bsln\n","\n","class MetaWindow:\n","  def __init__(self, support,query):\n","    # list of Windows \n","    self.support = support\n","    self.query = query\n","\n","  def add_support(self, win):\n","    self.support.append(win)\n","\n","  def add_query(self, win):\n","    self.query.append(win)\n","  \n","class MetaTaskData:\n","  def __init__(self, metawins):\n","    # include ONE metawin for EACH meta task. if there are n_way = 10, this list includes 10 metawins\n","    self.tasks_metawin = metawins\n","  def n_tasks(self):\n","    return len(self.tasks_metawin)\n","\n","class MAMLDataLoader:\n","    def __init__(self, X,y,y_bsln, meta_batch_size, n_way=10, k_shot = 1,q_query=1, n_step=1,n_rows=30):\n","        self.n_way = n_way\n","        self.k_shot = k_shot\n","        self.q_query = q_query\n","        self.n_rows = n_rows \n","\n","        self.X = X\n","        self.y = y\n","        self.y_bsln = y_bsln\n","        self.metawins = self.gen_metawins(k_shot,q_query,n_step,n_rows)\n","\n","        self.meta_batch_size = meta_batch_size\n","        self.steps = len(self.metawins) // meta_batch_size\n","\n","    def __len__(self):\n","        return len(self.metawins)\n","\n","    def get_one_metatask_data(self):\n","        \"\"\"\n","        Get a task, there are n_way classes in a task, each class has k_shot sheets for inner training, q_query sheets for outer training\n","        :return: support_data, query_data\n","        \"\"\"\n","        metawins_nway = random.sample(self.metawins, self.n_way)\n","        return MetaTaskData(metawins_nway)\n","\n","    def get_one_batch(self):\n","        \"\"\"\n","        Get a batch of samples, where a batch is based on tasks as individuals\n","        :return: k_shot_data, q_query_data\n","        k_shot_data: k-shot means the number of images for each class\n","        q_query_data: q-query means the number of images in test (meta-test)\n","\n","        \"\"\"\n","        while True:\n","            batch_tasks_data = []\n","            for _ in range(self.meta_batch_size):\n","                batch_tasks_data.append(self.get_one_metatask_data())\n","\n","            yield batch_tasks_data\n","\n","    def gen_metawins(self,k_shot,q_query,n_step,n_rows):\n","      win_list = []\n","      for idx in range(0,len(self.X), n_step):\n","        win_list.append(Window(X=self.X[idx:idx+n_rows], y=self.y[idx:idx+n_rows], y_bsln=self.y_bsln[idx:idx+n_rows]))\n","\n","      metawin_list = []\n","      n_metawins = len(win_list) - k_shot - q_query\n","\n","      for idx in range(n_metawins):\n","        s = [win_list[i] for i in range(idx, idx+k_shot)]\n","        q = [win_list[j] for j in range(idx+k_shot, idx+k_shot+q_query)]\n","        metawin_list.append(MetaWindow(support= s ,query=q))\n","\n","      random.shuffle(metawin_list)\n","      return metawin_list\n","\n","    def __str__(self):\n","      return f\" len(metwins):{len(self.metawins)},n_way:{self.n_way}, k_shot:{self.k_shot},q_query:{self.q_query}, n_rows:{self.n_rows }, len(X):{len(self.X)}, meta_batch_size:{self.meta_batch_size}\"\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"SV_v8JGOsdr7","executionInfo":{"status":"ok","timestamp":1676666700722,"user_tz":420,"elapsed":123,"user":{"displayName":"dev developer","userId":"15432253627082125706"}}},"outputs":[],"source":["# net.py\n","class Mish(tf.keras.layers.Layer):\n","  def __init__(self, **kwargs):\n","      super().__init__(**kwargs)\n","\n","  def call(self, inputs):\n","      return inputs * tf.math.tanh(tf.math.softplus(inputs))\n","\n","  def get_config(self):\n","      base_config = super().get_config()\n","      return {**base_config}\n","\n","  def compute_output_shape(self, input_shape):\n","      return input_shape\n","\n","\n","\n","class MAML:\n","    def __init__(self, input_shape, args):\n","        self.input_shape = input_shape\n","        self.args = args \n","        self.meta_model = self.get_maml_model()\n","    \n","\n","    def get_maml_model(self):\n","        from tcn import TCN\n","        model = tf.keras.models.Sequential([\n","        TCN(input_shape=(self.args.lookback_wind, self.args.n_feature), nb_filters=self.args.nb_filters, dilations=self.args.dilations,activation=Mish()),\n","        tf.keras.layers.Dense(1, activation=Mish())\n","        ])\n","        return model \n","\n","\n","    def train_on_batch(self,train_data, inner_optimizer, inner_step, outer_optimizer = None ):\n","      batch_acc = []\n","      batch_loss = []\n","      task_weights = []\n","      sup_loss = []\n","      sup_acc = []\n","      qu_loss = []\n","      qu_acc = [] \n","\n","\n","      # Save the initial weight with meta_weights and set it as the weight of the inner step model\n","      meta_weights = self.meta_model.get_weights()\n","\n","      metatask_data = next(train_data)[0]\n","      meta_support_X = [metatask_data.tasks_metawin[idx].support[idy].X  for idx in range(metatask_data.n_tasks()) for idy in range(len(metatask_data.tasks_metawin[idx].support))]\n","      meta_support_y = [metatask_data.tasks_metawin[idx].support[idy].y  for idx in range(metatask_data.n_tasks()) for idy in range(len(metatask_data.tasks_metawin[idx].support))]\n","      meta_query_X = [metatask_data.tasks_metawin[idx].query[idy].X  for idx in range(metatask_data.n_tasks()) for idy in range(len(metatask_data.tasks_metawin[idx].query))]\n","      meta_query_y = [metatask_data.tasks_metawin[idx].query[idy].y  for idx in range(metatask_data.n_tasks()) for idy in range(len(metatask_data.tasks_metawin[idx].query))]\n","\n","      for support_X, support_y in zip(meta_support_X, meta_support_y):\n","        # Each task needs to load the most original weights to update\n","        self.meta_model.set_weights(meta_weights)\n","        for _ in range(inner_step):\n","            with tf.GradientTape() as tape:\n","              logits = self.meta_model(support_X, training=True)\n","              loss = losses.mean_squared_error(support_y, logits)\n","              loss = tf.reduce_mean(loss)\n","              acc = metrics.mean_absolute_percentage_error (support_y,tf.transpose(logits)) \n","              acc = tf.reduce_mean(acc)\n","              sup_loss.append(loss)\n","              sup_acc.append(acc)\n","\n","            grads = tape.gradient(loss, self.meta_model.trainable_variables)\n","            inner_optimizer.apply_gradients(zip(grads, self.meta_model.trainable_variables))\n","\n","        # Every time the weights updated by the inner loop need to be saved once, to ensure that the same task is trained by the outer loop after the weights\n","        task_weights.append(self.meta_model.get_weights())\n","\n","      with tf.GradientTape() as tape:\n","        for i, (query_X, query_y) in enumerate(zip(meta_query_X, meta_query_y)):\n","          # Load each task weights for forward propagation\n","          self.meta_model.set_weights(task_weights[i])\n","          logits = self.meta_model(query_X, training=True)\n","          loss = losses.mean_squared_error(query_y, logits)\n","          loss = tf.reduce_mean(loss)\n","          batch_loss.append(loss)\n","          acc = metrics.mean_absolute_percentage_error (query_y,tf.transpose(logits) ) \n","          acc = tf.reduce_mean(acc)\n","          batch_acc.append(acc)\n","          qu_loss.append(loss)\n","          qu_acc.append(acc)\n","\n","        mean_acc = tf.reduce_mean(batch_acc)\n","        mean_loss = tf.reduce_mean(batch_loss)\n","\n","      # Whether it is updated or not, it is necessary to load the initial weight for update to prevent the val stage from changing the original weight\n","      self.meta_model.set_weights(meta_weights)\n","      \n","      if outer_optimizer:\n","        grads = tape.gradient(mean_loss, self.meta_model.trainable_variables)\n","        outer_optimizer.apply_gradients(zip(grads, self.meta_model.trainable_variables))\n","      \n","      return mean_loss, mean_acc, sup_loss, sup_acc, qu_loss, qu_acc\n","\n","def clone_maml_model(mo: MAML):\n","    new_model = MAML( input_shape = mo.input_shape, args = mo.args)\n","    return new_model\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"gRqTW4fRkAce","executionInfo":{"status":"ok","timestamp":1676666708807,"user_tz":420,"elapsed":140,"user":{"displayName":"dev developer","userId":"15432253627082125706"}}},"outputs":[],"source":["class Args: \n","  inner_lr=0.04\n","  outer_lr=0.001\n","  inner_optimizer=tfa.optimizers.LAMB(inner_lr)\n","  outer_optimizer=tfa.optimizers.LAMB(outer_lr)\n","\n","  nb_filters = 64\n","  dilations=(1, 2, 4, 8, 16,32)\n","  kernel_size =3\n","\n","  n_way=1\n","  inner_step = 1\n","  # inner_step = n_way\n","\n","  k_shot=5\n","  n_query=3\n","  meta_batch_size = 1\n","  n_rows = 7\n","  n_step = n_rows\n","  input_shape=(None,7)\n","\n","  n_feature = len(df_hlp.columns)-2\n","  lookback_wind=5\n","  epochs=10\n","\n","args = Args()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n33lfD0WyJ1Z"},"outputs":[],"source":["import time\n","from copy import deepcopy\n","\n","# k-shot means the number of windows in each support set \n","# n-query means the number of windows in each query set \n","\n","k_shot_s=[1,2,3,4,5,6,7,8,9,10]\n","n_query_s=[1,2,3,4,5,6,7,8,9,10]\n","\n","\n","\n","args.epochs=1\n","\n","maml = MAML(args.input_shape, args=args)\n","maml_c = deepcopy(maml)\n","\n","filename = time.strftime(\"%Y%m%d-%H%M%S\")\n","fp = open('maml-'+str(filename) + '.txt', 'w')\n","fp.close()\n","\n","\n","for k_shot in k_shot_s:\n","  for n_query in n_query_s:\n","    # for train_data_steps in train_data_steps_s:\n","    #   for val_data_steps in val_data_steps_s:\n","        args.k_shot = k_shot\n","        args.n_query = n_query\n","        print(f'args.k_shot:{args.k_shot}, args.n_query:{args.n_query}')\n","\n","\n","        X_hlp_lb, y_hlp_lb,y_bsln_hlp_lb = convert_to_lookback(df_hlp,lookback_wind= args.lookback_wind)\n","        X_inc_lb, y_inc_lb,y_bsln_inc_lb = convert_to_lookback(df_inc,lookback_wind= args.lookback_wind)\n","\n","        # print(f'X_hlp_lb.shape:{X_hlp_lb.shape},y_hlp_lb:{y_hlp_lb.shape}, y_y_bsln_hlp_lb:{y_bsln_hlp_lb.shape} ')\n","        # print(f'X_inc_lb.shape:{X_inc_lb.shape},y_inc_lb:{y_inc_lb.shape}, y_y_bsln_inc_lb:{y_bsln_inc_lb.shape} ')\n","\n","        args.meta_train ={'X':X_hlp_lb,'y':y_hlp_lb,'y_bsln':y_bsln_hlp_lb}\n","        args.meta_test={'X':X_inc_lb,'y':y_inc_lb,'y_bsln':y_bsln_inc_lb}  \n","\n","\n","        train_data = MAMLDataLoader(args.meta_train['X'],args.meta_train['y'],args.meta_train['y_bsln'], meta_batch_size=args.meta_batch_size,n_way = args.n_way,n_step= args.n_step, k_shot=args.k_shot , n_rows=args.n_rows)\n","        val_data = MAMLDataLoader(args.meta_test['X'],args.meta_test['y'],args.meta_test['y_bsln'], meta_batch_size=args.meta_batch_size,n_way = args.n_way,n_step= args.n_step, k_shot=args.k_shot , n_rows=args.n_rows)\n","        # number of tasks for training \n","        # number of tasks for validataion \n","        train_data.steps = 10\n","        val_data.steps = 1\n","\n","        maml = clone_maml_model(maml_c)\n","\n","        print(f'train_data.steps:{train_data.steps}, val_data.steps:{val_data.steps}')\n","\n","        sup_loss_li =[]\n","        sup_acc_li=[]\n","        qu_loss_li = []\n","        qu_acc_li = []\n","\n","        train_meta_loss = []\n","        train_meta_acc = []\n","        val_meta_loss = []\n","        val_meta_acc = []\n","\n","        for e in range(args.epochs):\n","            train_progbar = utils.Progbar(train_data.steps)\n","            val_progbar = utils.Progbar(val_data.steps)\n","            print('\\nEpoch {}/{}'.format(e+1, args.epochs))\n","\n","            # maml = clone_maml_model(maml_c)\n","\n","            for i in range(train_data.steps):\n","\n","                batch_train_loss, train_acc,_,_,_,_ = maml.train_on_batch(train_data.get_one_batch(), args.inner_optimizer, inner_step=args.inner_step, outer_optimizer=args.outer_optimizer)\n","\n","                train_meta_loss.append(batch_train_loss)\n","                train_meta_acc.append(train_acc)\n","                train_progbar.update(i+1, [('loss', np.mean(train_meta_loss)),('accuracy', np.mean(train_meta_acc))])\n","\n","            for i in range(val_data.steps):\n","                batch_val_loss, val_acc, sup_loss, sup_acc, qu_loss, qu_acc  = maml.train_on_batch(val_data.get_one_batch(), args.inner_optimizer, inner_step=args.inner_step)\n","\n","                val_meta_loss.append(batch_val_loss)\n","                val_meta_acc.append(val_acc)\n","                val_progbar.update(i+1, [('val_loss', np.mean(val_meta_loss)),('val_accuracy', np.mean(val_meta_acc))])\n","\n","                sup_loss_li.append(sup_loss)\n","                sup_acc_li.append(sup_acc)\n","                qu_loss_li.append(qu_loss)\n","                qu_acc_li.append(qu_acc)\n","\n","        qu_acc_res = [y.numpy() for x in qu_acc_li for y in x]\n","        qu_loss_res = [y.numpy() for x in qu_loss_li for y in x]\n","        sup_acc_res = [y.numpy() for x in sup_acc_li for y in x]\n","        sup_loss_res = [y.numpy() for x in sup_loss_li for y in x]\n","\n","        fp = open('maml-'+str(filename)+f'_{str(k_shot).zfill(2)}{str(n_query).zfill(2)}' + '.txt', 'w')\n","        fp.write(f'qu_acc_{k_shot}{n_query}={qu_acc_res} \\nqu_loss_{k_shot}{n_query}={qu_loss_res}\\nsup_acc_{k_shot}{n_query}={sup_acc_res},\\nsup_loss_{k_shot}{n_query}={sup_loss_res}\\n')\n","        fp.close()\n","\n","            # maml.meta_model.save_weights(\"maml.h5\")\n","# fp.close()\n","\n","print('... DONE ....')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2X3oEwA9Ybrv"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1ahrzBXJgsFrykyUYvt60_wTjSG3u8Y2c","timestamp":1662560266618}],"authorship_tag":"ABX9TyOa1RGY3peTCHyBaLg1PMwD"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}